{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d3e6614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2. Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Array2D\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# 3. Device check (MPS = Apple GPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ MPS not available, using CPU\")\n",
    "\n",
    "# 3. Load dataset\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "\n",
    "# 4. Load tokenizer and model\n",
    "model_name = \"mental/mental-bert-base-uncased\"   # MentalBERT base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "num_labels = 28   # 27 emotions + Neutral\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)   # move model to GPU if available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f868c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Tokenization\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "encoded = dataset.map(tokenize, batched=True)\n",
    "\n",
    "def one_hot_labels(batch):\n",
    "    multi_hot = []\n",
    "    for labels in batch[\"labels\"]:\n",
    "        label_vec = np.zeros(num_labels, dtype=np.float32)  # This is correct\n",
    "        for l in labels:\n",
    "            label_vec[l] = 1.0\n",
    "        multi_hot.append(label_vec)\n",
    "    batch[\"labels\"] = multi_hot\n",
    "    return batch\n",
    "\n",
    "# Re-run the mapping\n",
    "encoded = dataset.map(tokenize, batched=True)\n",
    "encoded = encoded.map(one_hot_labels, batched=True)\n",
    "\n",
    "# IMPORTANT: Explicitly specify dtype when setting format\n",
    "encoded.set_format(\n",
    "    type=\"torch\", \n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    output_all_columns=False\n",
    ")\n",
    "\n",
    "# Cast labels to float32 explicitly\n",
    "def cast_labels_to_float(example):\n",
    "    example[\"labels\"] = example[\"labels\"].float()\n",
    "    return example\n",
    "\n",
    "encoded = encoded.map(cast_labels_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f917f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(encoded[\"train\"][0][\"labels\"].shape, encoded[\"train\"][0][\"labels\"].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8244141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Metrics\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    labels = labels.astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"f1\": f1_metric.compute(predictions=preds.flatten(), \n",
    "                                references=labels.flatten(), \n",
    "                                average=\"micro\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "daaffcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# 3. Device check (MPS = Apple GPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ MPS not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9f6b03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this RIGHT AFTER your imports (at the top with other imports)\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "@dataclass\n",
    "class MultiLabelCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract labels - they're already tensors, so stack them\n",
    "        labels = torch.stack([f[\"labels\"] for f in features]).float()\n",
    "        \n",
    "        # Prepare batch for input_ids and attention_mask\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "        }\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f140f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/nl8hmv9n635ghbx5hw3ptb_00000gn/T/ipykernel_64307/2362485799.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8142' max='8142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8142/8142 12:28:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.088997</td>\n",
       "      <td>0.969571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.083399</td>\n",
       "      <td>0.970308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.083867</td>\n",
       "      <td>0.969920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8142, training_loss=0.09069550095949147, metrics={'train_runtime': 44933.023, 'train_samples_per_second': 2.898, 'train_steps_per_second': 0.181, 'total_flos': 8568237917583360.0, 'train_loss': 0.09069550095949147, 'epoch': 3.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mentalbert-goemotions\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # Reduce to 8 if you get memory errors on MPS\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,  # Only keep 2 best checkpoints\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded[\"train\"],\n",
    "    eval_dataset=encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=MultiLabelCollator(tokenizer=tokenizer),  # ADD THIS LINE\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14487a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "trainer.save_model(\"./mentalbert-goemotions-final\")\n",
    "tokenizer.save_pretrained(\"./mentalbert-goemotions-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b486c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = \"./mentalbert-goemotions-final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Create the pipeline\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,  # CPU\n",
    "    top_k=None  # Return all emotion scores\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3be25a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I'm so angry and frustrated about losing my job\n",
      "  anger: 0.639\n",
      "  annoyance: 0.396\n",
      "  disgust: 0.067\n",
      "\n",
      "Text: I feel hopeful and excited about my new opportunity\n",
      "  excitement: 0.529\n",
      "  optimism: 0.192\n",
      "  joy: 0.111\n",
      "\n",
      "Text: I'm terrified about being evicted next week\n",
      "  fear: 0.733\n",
      "  neutral: 0.057\n",
      "  nervousness: 0.055\n",
      "\n",
      "Text: I'm so stressed about rent and bills, can't sleep anymoreOkay so I have had toxic family in the sense of my dad and lots of other relatives but I fixed my dead and don‚Äôt give a fuck bout those relatives btw I‚Äôm Indian Punjabi specifically. So I had this cousin who seemed like a good guy until I found out he did drugs and did stuff like stealing from some big companies by means of fraud. So he wanted to go to India but the visa was closed down so he came to us and then we welcomed him and I saw he was much worse I was a clean respectable person who didn‚Äôt look at woman as eye candy like him scrolling on IG to keep seeing them and chasing girls I could insult him more but cause I have immense respect for his dad I don‚Äôt so yeah and like he wasted a lot of money on him to eat outside even though we don‚Äôt eat that much and got my dad drinking and as drunk fuck broke my dads nose then he as the audacity to call me a fat ass and think it‚Äôs fine to joke when it isn‚Äôt ( we are in India right now) and starts causing fights with me and when I accuse him of something every one believes him but not me sorry bout the gramma I‚Äôm tired and angry any ways my dad can‚Äôt work and my mom has to pull all the weight so I was like let‚Äôs do a side hustle to help her but my twisted of a fuck cousin stops me from finishing it and wrestles me down letting me go then tries to explain why it‚Äôs so so bad even though I know it‚Äôs a legit course than when I want be left alone he starts to play dad and explain to me when he‚Äôs no good so I left again what should I do for my goals also handle him and cause he makes fun of me (fyi why parents like him is cause mom left him with mine when he was 10 for 2 years.\n",
      "  annoyance: 0.494\n",
      "  disappointment: 0.209\n",
      "  anger: 0.199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define emotion label mapping\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', \n",
    "    'caring', 'confusion', 'curiosity', 'desire', 'disappointment',\n",
    "    'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness',\n",
    "    'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
    "    'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "# Now test with clearer examples\n",
    "test_texts = [\n",
    "    \"I'm so angry and frustrated about losing my job\",\n",
    "    \"I feel hopeful and excited about my new opportunity\",\n",
    "    \"I'm terrified about being evicted next week\",\n",
    "    \"I'm so stressed about rent and bills, can't sleep anymore\"\n",
    "    'Okay so I have had toxic family in the sense of my dad and lots of other relatives but I fixed my dead and don‚Äôt give a fuck bout those relatives btw I‚Äôm Indian Punjabi specifically. So I had this cousin who seemed like a good guy until I found out he did drugs and did stuff like stealing from some big companies by means of fraud. So he wanted to go to India but the visa was closed down so he came to us and then we welcomed him and I saw he was much worse I was a clean respectable person who didn‚Äôt look at woman as eye candy like him scrolling on IG to keep seeing them and chasing girls I could insult him more but cause I have immense respect for his dad I don‚Äôt so yeah and like he wasted a lot of money on him to eat outside even though we don‚Äôt eat that much and got my dad drinking and as drunk fuck broke my dads nose then he as the audacity to call me a fat ass and think it‚Äôs fine to joke when it isn‚Äôt ( we are in India right now) and starts causing fights with me and when I accuse him of something every one believes him but not me sorry bout the gramma I‚Äôm tired and angry any ways my dad can‚Äôt work and my mom has to pull all the weight so I was like let‚Äôs do a side hustle to help her but my twisted of a fuck cousin stops me from finishing it and wrestles me down letting me go then tries to explain why it‚Äôs so so bad even though I know it‚Äôs a legit course than when I want be left alone he starts to play dad and explain to me when he‚Äôs no good so I left again what should I do for my goals also handle him and cause he makes fun of me (fyi why parents like him is cause mom left him with mine when he was 10 for 2 years.'\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = emotion_classifier(text)\n",
    "    top_3 = sorted(result[0], key=lambda x: x['score'], reverse=True)[:3]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    for r in top_3:\n",
    "        emotion = emotion_labels[int(r['label'].split('_')[1])]\n",
    "        print(f\"  {emotion}: {r['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d877785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
